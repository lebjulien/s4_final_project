---
title: "Projet final Test 2"
output:
  html_document: 
    code_folding: hide
  pdf_document: default
date: "2023-04-24"
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Partie I

Téléchargement de notre dataset pour lequel nous avons renommé les noms des colonnes.

```{r}
data <- read_csv("wdbc.data", 
                 col_names = c("ID number",
                               "Diagnosis",
                               "radius_mean",
                               "texture_mean",
                               "perimeter_mean",
                               "area_mean","smoothness_mean",
                               "compactness_mean",
                               "concavity_mean",
                               "concave_points_mean",
                               "symmetry_mean",
                               "fractal_dimension_mean",
                               "radius_SE","texture_SE",
                               "perimeter_SE","area_SE",
                               "smoothness_SE",
                               "compactness_SE",
                               "concavity_SE",
                               "concave_points_SE",
                               "symmetry_SE",
                               "fractal_dimension_SE",
                               "radius_worst",
                               "texture_worst",
                               "perimeter_worst",
                               "area_worst",
                               "smoothness_worst",
                               "compactness_worst",
                               "concavity_worst",
                               "concave_points_worst",
                               "symmetry_worst",
                               "fractal_dimension_worst"))
```

Liste des packages nécessaires 

```{r}
library(readr)
library(tidyverse)
library(readxl)
library(FactoMineR)
library(factoextra)
library(arsenal)
library(knitr)
library(dplyr)
library(ggplot2)
library(clean)
library(gtsummary)
library(clValid)
library(mclust)
library(ggpubr)
source(url("https://raw.githubusercontent.com/larmarange/JLutils/master/R/clustering.R"))
```

## Question n°1

```{r}
glimpse(data)
data$Diagnosis <- as.factor(data$Diagnosis)
summary(data)
```


## Question n°2

### 1 - Analyse descriptive du jeu de donnée par clustering K-means

Pour réaliser cette analyse descriptive, nous allons réaliser une analyse par clustering en utilisant la méthode des k plus proches voisins.

Dans un premier temps, il est alors nécessaire de charger et de nettoyer le jeu de données en renommant les variables et en éliminant les données manquantes. Il est aussi nécessaire de renommer les différentes variables présentes dans le dataset. Ainsi, les variables se terminant par \_mean réfèrent à la moyenne de celle-ci, les variables se terminant par \_SE indiquent la Mean Squared Error et celle terminant par \_worst indiquent la pire évaluation.

```{r, message=FALSE}
clean_data <- data %>% 
  select(c(contains("_mean"), Diagnosis)) %>% 
  drop_na()

km_dataset <- data %>% 
  drop_na()
```

Le paramétrage de la seed ci-dessous permet d'obtenir une reproductibilité dans les résultats obtenus lors des entraînements de modèles.

```{r}
set.seed(123)
```

Par choix, nous allons choisir de nous concentrer sur les variables terminant par \_mean afin d'obtenir une certaine constance dans les résultats et leurs interprétabilité.

Dans le but de déterminer le nombre optimal de clusters, nous allons utiliser l'Elbow method qui consiste à tracer la somme des carrés des distances intracluster (WCSS) en fonction du nombre de clusters et à rechercher le point où la courbe forme un "coude" ou un changement de direction brusque.

Ainsi dans la courbe obtenue ci-dessous, on peut observer que la cassure a lieu pour 2 clusters. Voici donc notre nombre optimal de clusters.

```{r}
# Fancy K-Means
fviz_nbclust(scale(km_dataset[,3:12]), kmeans, nstart=100, method = "wss") + 
  geom_vline(xintercept = 2, linetype = 1)
```

La méthode utilisée ici est que le modèle va créer une dataset clone de celui initial et va y ajouter une variable supplémentaire nommée 'Cluster' et pour chaque ligne y indiquer dans quels clusters elle se trouve.

```{r}
kmeans_basic <- kmeans(km_dataset[,3:12], centers = 2)
kmeans_basic_table <- data.frame(kmeans_basic$size, kmeans_basic$centers)
kmeans_basic_df <- data.frame(Cluster = kmeans_basic$cluster, km_dataset)

kable(kmeans_basic_df[1:6, 1:7], 
      format = "latex",
      booktabs = T)
```

On peut alors représenter les deux clusters créés puis y indiquer en leurs seins s'il s'agit du diagnostic bénin (B) ou alors malin (M).

Dans ce graphique, on peut alors observer que le cluster 2 est quasiment totalement constitué de patients dont la tumeur a été diagnostiquée comme maligne tandis que pour le cluster 1, on peut remarquer qu'il y a une nette disparité dans la composition du cluster.

```{r}
# Example ggplot
ggplot(data = kmeans_basic_df, aes(x = Cluster)) +
  geom_bar(aes(fill = Diagnosis)) +
  ggtitle("Count of Clusters by Diagnosis") +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_fill_brewer(palette = "Set1")

```

On peut aussi observer ci-dessous la représentation graphique des deux clusters obtenus.

```{r}
fviz_cluster(kmeans_basic, data = scale(km_dataset[,3:12]), geom = c("point"),ellipse.type = "euclid")
```

Dans cette seconde partie de la génération du modèle de clustering par k-mean, nous allons essayer d'améliorer le modèle que nous avions précédemment obtenu. Pour cela, nous allons augmenter le nombre d'essais de génération du clustering en passant à 100 le nombre de positions aléatoire de départ du K.

```{r}
# Fancy kmeans
set.seed(123)
kmeans_fancy <- kmeans(scale(km_dataset[,3:12]), 2, nstart = 100)

kmeans_fancy_df <- data.frame(Cluster = kmeans_fancy$cluster, km_dataset)

# plot the clusters
fviz_cluster(kmeans_fancy, data = scale(km_dataset[,3:12]), geom = c("point"),ellipse.type = "euclid")
```

On observe pour ce nouveau modèle qu'un des clusters reste majoritairement composé de patients dont la tumeur est maligne tandis que pour le second cluster, la proportion de patient dont la tumeur est maligne diminue comparé au précédent modèle. On peut alors en conclure que le second modèle est plus précis pour partitionner les deux classes de patients.

```{r}

ggplot(data = kmeans_fancy_df, aes(x = Cluster)) +
  geom_bar(aes(fill = Diagnosis)) +
  ggtitle("Count of Clusters by Diagnosis") +
  theme(plot.title = element_text(hjust = 0.5)) + 
  scale_fill_brewer(palette = "Set1")
```

Enfin, dans le tableau ci-dessous, nous réalisons une étude statistique de la composition des clusters.

```{r results='asis'}
outCtl <- tableby(Cluster ~ Diagnosis + radius_mean + texture_mean + 
                    perimeter_mean + area_mean + smoothness_mean + 
                    compactness_mean + concavity_mean + concave_points_mean + 
                    symmetry_mean + fractal_dimension_mean, 
                  data=kmeans_fancy_df,
                  control=tableby.control(total=T, cat.simplify=F, 
                  numeric.stats = c("Nmiss", "meansd", "range"),digits=1))

summary(outCtl, text=F)
```

### 2 - Analyse descriptive du jeu de donnée par clustering Classification ascendante hiérarchique

```{r}
wdbc <- read_csv("wdbc.data", 
                 col_names = c("ID number",
                               "Diagnosis",
                               "radius_mean",
                               "texture_mean",
                               "perimeter_mean",
                               "area_mean","smoothness_mean",
                               "compactness_mean",
                               "concavity_mean",
                               "concave_points_mean",
                               "symmetry_mean",
                               "fractal_dimension_mean",
                               "radius_SE","texture_SE",
                               "perimeter_SE","area_SE",
                               "smoothness_SE",
                               "compactness_SE",
                               "concavity_SE",
                               "concave_points_SE",
                               "symmetry_SE",
                               "fractal_dimension_SE",
                               "radius_worst",
                               "texture_worst",
                               "perimeter_worst",
                               "area_worst",
                               "smoothness_worst",
                               "compactness_worst",
                               "concavity_worst",
                               "concave_points_worst",
                               "symmetry_worst",
                               "fractal_dimension_worst"))
```


Nous sélectionnons les variables qui nous interessent, ici tous les variables _mean, et faisons une standardisation. Nous faisons ensuite un calcul de la matrice de distances entre les individus en utilisant la méthode de calcul euclidienne.
```{r}
# Sélection des variables et standardisation
myvars <- wdbc[,c("radius_mean", "texture_mean", "perimeter_mean","area_mean","smoothness_mean","compactness_mean","concavity_mean","concave_points_mean","symmetry_mean","fractal_dimension_mean")]
myvars <- scale(myvars)

# Calcul de la matrice de distances
mydist <- dist(myvars, method = "euclidean")
mydist


```

Nous effectuons maintenant la classification ascendante hiérarchique en utilisant la méthode de Ward.Nous obtenons ainsi un dendrogramme représentant la classification obtenue.

```{r}
# Classification ascendante hiérarchique
myhclust <- hclust(mydist, method = "ward.D2")

# Visualisation du dendrogramme
plot(myhclust, labels = FALSE)
```

Nous allons utiliser des visualisations pour interpréter les résultats de l'analyse de clustering et prendre des décisions concernant le nombre de classes à retenir. Nous pouvons voir que nous observons qu'il est possible de définir 2 ou 3 classes suite à la classification.
```{r}
inertie <- sort(myhclust$height, decreasing = TRUE)
plot(inertie[1:20], type = "s", xlab = "Nombre de classes", ylab = "Inertie")
points(c(2, 3), inertie[c(2, 3)], col = c("green3", "red3"), cex = 2, lwd = 3)

plot(myhclust, labels = FALSE, main = "Partition en 2 ou 3 classes", xlab = "", ylab = "", sub = "", axes = FALSE, hang = -1)
rect.hclust(myhclust, 2, border = "green3")
rect.hclust(myhclust, 3, border = "red3")

fviz_dend(myhclust, k = 2, show_labels = FALSE, rect = TRUE)
fviz_dend(myhclust, k = 3, show_labels = FALSE, rect = TRUE)

```

Nous utilisons une fonction nous permettant de déterminer la meilleure coupe du dendrogramme.Nous obtenons comme résultat que le nombre de classes optimal est 3.

```{r}
best.cutree(myhclust)

best.cutree(myhclust, graph = TRUE, xlab = "Nombre de classes", ylab = "Inertie relative")

typo <- cutree(myhclust, 3)
freq(typo)

```

Nous prenons donc comme valeur k= 3 et ajoutons une variable supplémentaire nommée 'Cluster' et pour chaque ligne y indiquer dans quels clusters elle se trouve.

```{r}
# Détermination du nombre de groupes à former
mycut <- cutree(myhclust, k = 3)

# Attribution des individus aux groupes correspondants
wdbc$Cluster <- mycut

hc.cut <- hcut(myvars, k= 3, hc_method = "complete")
fviz_cluster(hc.cut, ellipse.type = "convex")
```

Nous utilisons l'indice de Rand ajusté, pour évaluer objectivement la qualité de notre clustering en le comparant aux véritables classes des données. Cela nous permet de mesurer à quel point les groupes obtenus correspondent aux structures réelles des données.Nous obtenons Une valeur de 0,47, qui indique une concordance relativement faible entre les partitions du clustering et les étiquettes de classes.

```{r}
rst <- adjustedRandIndex(mycut, wdbc$Diagnosis) 
rst
```


## Question 3

Nous allons ensuite développer différents modèles de diagnostic et en évaluer leurs performances. Trois approches vont être utilisées : une méthode par Arbre de décision, une par Forêts aléatoires et une par Ensemble Learning.

### 1 - Méthode par Arbre de décision

Pour cette méthode par Arbre de décision, nous allons commencer par charger une dataset clone des données que nous souhaitons analyser. Nous allons ensuite partitionner ce dataset en deux parties : une qui servira de base de données d'entrainement du modèle, et une qui nous permettra d'évaluer le modèle obtenu précédemment.

```{r, message=FALSE}
library(tidyverse)
library(FactoMineR)
library(factoextra)
library(rpart)
library(rpart.plot)
library(caret)

dt_dataset <- clean_data

nb_lignes <- floor((nrow(dt_dataset)*0.75)) #Nombre de lignes de l’échantillon d’apprentissage : 75% du dataset
dt_dataset <- dt_dataset[sample(nrow(dt_dataset)), ] #Ajout de numéros de lignes
dt_dataset.train <- dt_dataset[1:nb_lignes, ] #Echantillon d’apprentissage
dt_dataset.test <- dt_dataset[(nb_lignes+1):nrow(dt_dataset), ] #Echantillon de test
```

Voici ci-dessous les résultats du modèle entraîné sur le dataset d'entraînement. Cet arbre de décision est volumineux et va donc avoir besoin d'un élagage afin de réduire sa complexité. Pour cela, nous allons faire appel à la formule du coût de complexité. Il s'agit d'un paramètre qui permet de contrôler la taille maximale de l'arbre de décision, c'est-à-dire le nombre maximal de nœuds ou de feuilles dans l'arbre.

En général, un arbre de décision plus complexe peut mieux s'adapter aux données d'entraînement, mais il est également plus susceptible de surapprendre (overfitting) et de mal généraliser aux nouvelles données. Par conséquent, le coût de complexité est souvent utilisé pour éviter le surapprentissage en régularisant le modèle et en limitant sa complexité.

```{r}
set.seed(12)
#Construction de l’arbre
dataset.Tree <- rpart(Diagnosis ~ ., 
                      data = dt_dataset.train,
                      method = "class", 
                      control = rpart.control(minsplit = 5, cp=0))

#Affichage du résultat
rpart.plot(dataset.Tree)
```

Pour bien élaguer notre arbre de décision, nous allons chercher le coût de complexité (cp) pour lequel le taux de mauvais classement (xerror) est la plus faible.

```{r fig.width=4, fig.asp=0.8}
#On cherche à minimiser l’erreur pour définir le niveau d’élagage
#plotcp(dataset.Tree)
printcp(dataset.Tree)
```

Le coût de complexité optimal pour ce modèle va s'afficher sous ce paragraphe. Ce paramètre va ensuite être utilisé pour élaguer l'arbre obtenu précédemment.

```{r}
print(dataset.Tree$cptable[which.min(dataset.Tree$cptable[,4]),1])
```

Voici donc ci-dessous le résultat de l'élagage de l'arbre de décision. Ce nouvel arbre est plus court et donc, plus généraliste et aura par conséquence moins tendance à overfitter.

```{r}
set.seed(12)
#Elagage de l’arbre avec le cp optimal
dataset.Tree_Opt <- prune(dataset.Tree,
                          cp = dataset.Tree$cptable[which.min(dataset.Tree$cptable[,4]),1])

#Représentation graphique de l’arbre optimal
rpart.plot(dataset.Tree_Opt)
```

Enfin, nous avons entrainé le modèle obtenu sur le dataset de test que nous avions précédemment créé. Ensuite, nous avons réalisé plusieurs tests statistiques sur les prédictions qu'il a pu faire.

Les résultats de l'arbre de décision montrent une performance globalement élevée, avec une précision (accuracy) de 91,61%. On peut être satisfait de la capacité du modèle à discriminer entre les classes, avec une sensibilité de 84% pour la classe positive (M) et une spécificité de 95,7% pour la classe négative (B). Le coefficient Kappa de 0,812 indique un accord significatif entre les prédictions du modèle et les valeurs réelles. Ces résultats me donnent confiance dans la compétence du modèle à effectuer des prédictions précises.

```{r}
#Prédiction du modèle sur les données de test
dataset.test_Predict<-predict(dataset.Tree_Opt,newdata=dt_dataset.test, type= "class")

dt_dataset.test$Diagnosis <- factor(dt_dataset.test$Diagnosis, levels = c("B", "M"))

#Création d'un tableau de confusion
confusionMatrix(dt_dataset.test$Diagnosis, dataset.test_Predict, positive = "M")
```

### 2 - Méthode par Forets aléatoires

```{r echo=FALSE, message=FALSE}
library(readr)
library(randomForest)
library(plotly)

rf_dataset <- clean_data

rf_dataset$Diagnosis <- factor(rf_dataset$Diagnosis)
```

Pour cette méthode par Forêts aléatoires (Random forests), il est nécessaire de partitionner notre dataset initial en deux sections : une d'entraînement et une de test.

```{r}
# Diviser les données en ensembles d'apprentissage et de test
set.seed(123) # pour la reproductibilité des résultats

train_index <- sample(nrow(rf_dataset), 0.7 * nrow(rf_dataset))
train_data <- rf_dataset[train_index, ]
test_data <- rf_dataset[-train_index, ]
```

Nous allons ensuite entrainer le modèle de Random forests.

```{r}
# Entraîner le modèle de forêt aléatoire
rf_model <- randomForest(Diagnosis ~ ., data = train_data, ntree = 100, mtry = 2, na.action = na.omit)

# Afficher les résultats du modèle
print(rf_model)
```

Puis faire en sorte d'afficher les variables que le modèle considère comme importantes et indiquer par un score si elles sont plus ou moins importantes.

Les variables les plus importantes sont celles qui ont les valeurs les plus élevées de "MeanDecreaseGini". Dans notre cas, les variables les plus importantes sont "concave_points_mean" avec une valeur de 38.963054, suivie par "area_mean" avec 27.354739 et "perimeter_mean" avec 25.170804.

Les variables ayant des valeurs relativement faibles de "MeanDecreaseGini" sont considérées comme moins importantes pour la prédiction dans ce modèle. Ici, les variables "symmetry_mean" et "fractal_dimension_mean" ont les valeurs les plus faibles avec 3.806719 et 4.201104 respectivement et présentent donc l'importance la plus faible.

```{r}
# Calculer l'importance des variables
var_importance <- importance(rf_model)

# Afficher les variables les plus importantes
print(var_importance)
```

Enfin nous allons soumettre notre modèle au dataset de test afin qu'il puissent réaliser ses prédictions. Ses résultats seront analysés grace à différents tests statistiques.

En résumé, notre modèle Random Forest semble présenter de bons résultats avec une précision globale de 91,81%. Il démontre une sensibilité élevée (94,03%) et une bonne spécificité (90,38%). Le coefficient Kappa de 0,8309 indique un accord entre les prédictions du modèle et les étiquettes réelles.

```{r}
# Évaluer la performance du modèle sur les données de test
rf_predictions <- predict(rf_model, test_data)
confusionMatrix(test_data$Diagnosis, rf_predictions, positive = "M")
```

### 3 - Méthode par Ensemble Learning

```{r echo=FALSE, message=FALSE}
library(tidyverse)
library(mlbench)
library(caret)
library(caretEnsemble)

el_dataset <- clean_data
```

Pour cette méthode par Ensemble Learning, nous allons utiliser 3 techniques différentes afin d'agréger les différents algorithmes qui seront utilisés. Nous exploiterons dans un premier temps un algorithme de Boosting, puis de Bagging et enfin de Stacking.

Les algorithmes par Boosting font en sorte que construire plusieurs modèles qui vont fixer les erreurs de prédictions du précédent modèle dans la chaine. Ceux par Bagging vont construire différents modèles sur différentes partitions du dataset initial. Enfin ceux par Stacking construisent différents modèles et un modèle de supervision qui va apprendre comment combiner ces modèles primaires.

Ainsi ci-dessous, nous allons agréger un algorithme nommé C5.0 et un algorithme appelé Stochastic Gradient Boosting (GBM). Les résultats affichés ci-dessous indiquent que le GBM est légèrement supérieur au C5.0 avec une précision (Accuracy) de 95.07% contre 95.01% et un Kappa (score permettant d'évaluer la performance d'un modèle) de 89.41% contre 89.32%.

```{r}
# Example of Boosting Algorithms
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 7
metric <- "Accuracy"

# C5.0
set.seed(seed)
fit.c50 <- train(Diagnosis~., data=clean_data, method="C5.0", metric=metric, trControl=control)

# Stochastic Gradient Boosting
set.seed(seed)
fit.gbm <- train(Diagnosis~., data=clean_data, method="gbm", metric=metric, trControl=control, verbose=FALSE)

# summarize results
boosting_results <- resamples(list(c5.0=fit.c50, gbm=fit.gbm))
summary(boosting_results)
dotplot(boosting_results)
```

Ensuite, dans le cadre des algorithmes de Bagging, nous allons utiliser deux algorithmes différents : le Bagged CART et les Forêts aléatoires (Random forests). Les résultats obtenus indiquent que le modèle basé sur le Random forests est supérieur au Bagged CART avec une précision de 94.31% contre 94.01% et un Kappa de 87.85% contre 87.21%.

```{r}
# Example of Bagging algorithms
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 7
metric <- "Accuracy"

# Bagged CART
set.seed(seed)
fit.treebag <- train(Diagnosis~., data=clean_data, method="treebag", metric=metric, trControl=control)

# Random Forest
set.seed(seed)
fit.rf <- train(Diagnosis~., data=clean_data, method="rf", metric=metric, trControl=control)

# summarize results
bagging_results <- resamples(list(treebag=fit.treebag, rf=fit.rf))
summary(bagging_results)
dotplot(bagging_results)
```

Pour la méthode par Stacking, nous utilisons 5 algorithmes : Linear Discriminate Analysis (LDA), Classification and Regression Trees (CART), la Régression logistique, les K plus proches voisins et Support Vector Machine with a Radial Basis Kernel Function (SVM).

Les résultats indiquent que le SVM produit le modèle le plus précis avec une précision de 95.02% et un Kappa de 89.22%.

```{r message=FALSE, warning=FALSE}
# Example of Stacking algorithms
# create submodels
control <- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)
algorithmList <- c('lda', 'rpart', 'glm', 'knn', 'svmRadial')
set.seed(seed)
models <- caretList(Diagnosis~., data=clean_data, trControl=control, methodList=algorithmList)
results <- resamples(models)
summary(results)
dotplot(results)
```

On peut observer, avant de stacker les modèles, que LDA présente de fortes corrélations avec GLM (0.8 de corrélation avec un seuil significatif à 0.75) et avec SVM (0.77 de corrélations).

```{r }
# correlation between results
modelCor(results)
splom(results)
```

Étant donné les trop fortes corrélations avec ces trois algorithmes, nous allons donc exclure LDA afin d'éviter de fausser notre modèle par Stacking. On peut alors observer que par ce Stacking par GLM, notre précision a diminué de 95.02% à 94.49% et le Kappa a de même diminué de 89.22% à 88.15%.

La conclusion de cette technique de Stacking par GLM est donc que notre modèle par SVM apportait de meilleures performances sans Stacking.

```{r message=FALSE, warning=FALSE}
# stack using glm
new_algorithmList <- c('glm', 'rpart', 'knn', 'svmRadial')
new_models <- caretList(Diagnosis~., data=clean_data, trControl=control, methodList=new_algorithmList)


stackControl <- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)
set.seed(seed)
stack.glm <- caretStack(new_models, method="glm", metric="Accuracy", trControl=stackControl)
print(stack.glm)
```

Tandis que pour le Stacking par Random Forests, notre précision augmente en passant à 96.21% et le Kappa augmente aussi pour atteindre 91.82%.

On peut donc affirmer que le Stacking par Random Forests apporte les meilleures performances de modèles par Ensemble Learning.

```{r}
# stack using random forest
set.seed(seed)
stack.rf <- caretStack(new_models, method="rf", metric="Accuracy", trControl=stackControl)
print(stack.rf)
```

# Partie II

Nous avons choisi comme de notre choix un dataset sur le diabète et nous avons fais le choix de choisir une approche supervisée d'arbre de décision.
Dataset : https://www.kaggle.com/datasets/iammustafatz/diabetes-prediction-dataset?resource=download&fbclid=IwAR1F9jDfOzNJTqTne1SHU1kka85rcS9atS6BihGSZFTxHp80T2t9Egps5YU


Nous commencons par nettoyer notre dataset et vérifier que nos variables sont bien définies sous la bonne catégorie de variables : ici facteur pour 6 de nos variables.
```{r}

diabetes_prediction_dataset$gender [diabetes_prediction_dataset$gender == "Other"] <- NA
diabetes_prediction_dataset$smoking_history [diabetes_prediction_dataset$smoking_history == "No Info"] <- NA

```

```{r}
glimpse(diabetes_prediction_dataset)
diabetes_prediction_dataset$gender <- as.factor(diabetes_prediction_dataset$gender)
diabetes_prediction_dataset$hypertension <- as.factor(diabetes_prediction_dataset$hypertension)
diabetes_prediction_dataset$heart_disease <- as.factor(diabetes_prediction_dataset$heart_disease)
diabetes_prediction_dataset$heart_disease <- as.factor(diabetes_prediction_dataset$heart_disease)
diabetes_prediction_dataset$smoking_history <- as.factor(diabetes_prediction_dataset$smoking_history)
diabetes_prediction_dataset$diabetes <- as.factor(diabetes_prediction_dataset$diabetes)
summary(diabetes_prediction_dataset)
        

```

Pour cette méthode par Arbre de décision, nous allons partionner ce dataset en deux parties : une qui servira de base de données d'entrainement du modèle, et une qui nous permettra d'évaluer le modèle obtenu précédemment.
```{r}
#Création d’un dataset d’apprentissage et d’un dataset de validation
nb_lignes <- floor((nrow(diabetes_prediction_dataset)*0.75)) #Nombre de lignes de l’échantillon d’apprentissage : 75% du dataset
diabetes_prediction_dataset <- diabetes_prediction_dataset[sample(nrow(diabetes_prediction_dataset)), ] #Ajout de numéros de lignes
diabetes.train <- diabetes_prediction_dataset[1:nb_lignes, ] #Echantillon d’apprentissage
diabetes.test <- diabetes_prediction_dataset[(nb_lignes+1):nrow(diabetes_prediction_dataset), ] #Echantillon de test
```

Voici ci-dessous les résultats du modèle entraîné sur le dataset d'entraînement. Cet arbre de décision est volumineux et va donc avoir besoin d'un élagage afin de réduire sa complexité. Pour cela, nous allons faire appel à la formule du coût de complexité. Il s'agit d'un paramètre qui permet de contrôler la taille maximale de l'arbre de décision, c'est-à-dire le nombre maximal de nœuds ou de feuilles dans l'arbre.

En général, un arbre de décision plus complexe peut mieux s'adapter aux données d'entraînement, mais il est également plus susceptible de surapprendre (overfitting) et de mal généraliser aux nouvelles données. Par conséquent, le coût de complexité est souvent utilisé pour éviter le surapprentissage en régularisant le modèle et en limitant sa complexité.

```{r}
set.seed(12)
#Construction de l’arbre
diabetes.Tree <- rpart(diabetes~.,
                       data=diabetes.train,
                       method= "class",
                       control=rpart.control(minsplit=8,cp=0))

#Affichage du résultat
rpart.plot(diabetes.Tree)

#On cherche à minimiser l’erreur pour définir le niveau d’élagage
plotcp(diabetes.Tree)
printcp(diabetes.Tree)

print(diabetes.Tree$cptable[which.min(diabetes.Tree$cptable[,4]),1])

set.seed(12)
#Elagage de l’arbre avec le cp optimal
diabetes.Tree_Opt <- prune(diabetes.Tree,cp=diabetes.Tree$cptable[which.min(diabetes.Tree$cptable[,4]),1])

prp(diabetes.Tree_Opt,extra=1)
rpart.plot(diabetes.Tree_Opt)
```

Pour bien élaguer notre arbre de décision, nous allons chercher le coût de complexité (cp) pour lequel le taux de mauvais classement (xerror) est la plus faible.

```{r}
#On cherche à minimiser l’erreur pour définir le niveau d’élagage
plotcp(diabetes.Tree)
printcp(diabetes.Tree)
```

Le coût de complexité optimal pour ce modèle va s'afficher sous ce paragraphe. Ce paramètre va ensuite être utilisé pour élaguer l'arbre obtenu précédemment.

```{r}
print(diabetes.Tree$cptable[which.min(diabetes.Tree$cptable[,4]),1])
```

Voici donc ci-dessous le résultat de l'élagage de l'arbre de décision. Ce nouvel arbre est plus court et donc, plus généraliste et aura par conséquence moins tendance à overfitter.

```{r}
set.seed(12)
#Elagage de l’arbre avec le cp optimal
diabetes.Tree_Opt <- prune(diabetes.Tree,cp=diabetes.Tree$cptable[which.min(diabetes.Tree$cptable[,4]),1])

#Représentation graphique de l’arbre optimal
rpart.plot(diabetes.Tree_Opt)
```

Enfin, nous avons entrainé le modèle obtenu sur le dataset de test que nous avions précédemment créé. Ensuite, nous avons réalisé plusieurs tests statistiques sur les prédictions qu'il a pu faire. 

Les résultats de l'arbre de décision montrent une performance globalement élevée, avec une précision (accuracy) de 97,2%. On peut être satisfait de la capacité du modèle à discriminer entre les classes, avec une sensibilité de 100% pour la classe positive (1) et une spécificité de 97,03% pour la classe négative (0). Le coefficient Kappa de 0,788 indique un accord significatif entre les prédictions du modèle et les valeurs réelles. Ces résultats me donnent confiance dans la compétence du modèle à effectuer des prédictions précises.
```{r}
#Prédiction du modèle sur les données de test
diabetes.test_Predict<-predict(diabetes.Tree_Opt,newdata=diabetes.test, type= "class")


#Création d'un tableau de confusion
confusionMatrix(diabetes.test$diabetes, diabetes.test_Predict, positive = "1")

