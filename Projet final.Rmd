---
title: "Projet final Test 2"
output:
  html_document: default
  pdf_document: default
date: "2023-04-24"
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question n°2

### 1 - Analyse descriptive du jeu de donnée par clustering

Pour réaliser cette analyse descriptive, nous allons réaliser une analyse par clustering en utilisant la méthode des k plus proches voisins.

Dans un premier temps, il est alors nécessaire de charger et de nettoyer le jeu de données en renommant les variables et en éliminant les données manquantes. Il est aussi nécessaire de renommer les différentes variables présentes dans le dataset. Ainsi, les variables se terminant par \_mean réfèrent à la moyenne de celle-ci, les variables se terminant par \_SE indiquent la Mean Squared Error et celle terminant par \_worst indiquent la pire évaluation.

```{r, echo=FALSE, message=FALSE}
library(readr)
library(tidyverse)
library(readxl)
library(FactoMineR)
library(factoextra)
library(arsenal)
library(knitr)

data <- read_csv("wdbc.data", 
                 col_names = c("ID number",
                               "Diagnosis",
                               "radius_mean",
                               "texture_mean",
                               "perimeter_mean",
                               "area_mean","smoothness_mean",
                               "compactness_mean",
                               "concavity_mean",
                               "concave_points_mean",
                               "symmetry_mean",
                               "fractal_dimension_mean",
                               "radius_SE","texture_SE",
                               "perimeter_SE","area_SE",
                               "smoothness_SE",
                               "compactness_SE",
                               "concavity_SE",
                               "concave_points_SE",
                               "symmetry_SE",
                               "fractal_dimension_SE",
                               "radius_worst",
                               "texture_worst",
                               "perimeter_worst",
                               "area_worst",
                               "smoothness_worst",
                               "compactness_worst",
                               "concavity_worst",
                               "concave_points_worst",
                               "symmetry_worst",
                               "fractal_dimension_worst"))

clean_data <- data %>% 
  select(c(contains("_mean"), Diagnosis)) %>% 
  drop_na()

km_dataset <- data %>% 
  drop_na()
```

Le paramétrage de la seed ci-dessous permet d'obtenir une reproductibilité dans les résultats obtenus lors des entraînements de modèles.

```{r}
set.seed(123)
```

Par choix, nous allons choisir de nous concentrer sur les variables terminant par \_mean afin d'obtenir une certaine constance dans les résultats et leurs interprétabilité.

Dans le but de déterminer le nombre optimal de clusters, nous allons utiliser l'Elbow method qui consiste à tracer la somme des carrés des distances intracluster (WCSS) en fonction du nombre de clusters et à rechercher le point où la courbe forme un "coude" ou un changement de direction brusque.

Ainsi dans la courbe obtenue ci-dessous, on peut observer que la cassure a lieu pour 2 clusters. Voici donc notre nombre optimal de clusters.

```{r, echo=FALSE, fig.width=4, fig.asp=0.8}
# Fancy K-Means
fviz_nbclust(scale(km_dataset[,3:12]), kmeans, nstart=100, method = "wss") + 
  geom_vline(xintercept = 2, linetype = 1)
```

La méthode utilisée ici est que le modèle va créer une dataset clone de celui initial et va y ajouter une variable supplémentaire nommée 'Cluster' et pour chaque ligne y indiquer dans quels clusters elle se trouve.

```{r tidy=TRUE}
kmeans_basic <- kmeans(km_dataset[,3:12], centers = 2)
kmeans_basic_table <- data.frame(kmeans_basic$size, kmeans_basic$centers)
kmeans_basic_df <- data.frame(Cluster = kmeans_basic$cluster, km_dataset)

kable(kmeans_basic_df[1:6, 1:7], 
      format = "latex",
      booktabs = T)
```

On peut alors représenter les deux clusters créés puis y indiquer en leurs seins s'il s'agit du diagnostic bénin (B) ou alors malin (M).

Dans ce graphique, on peut alors observer que le cluster 2 est quasiment totalement constitué de patients dont la tumeur a été diagnostiquée comme maligne tandis que pour le cluster 1, on peut remarquer qu'il y a une nette disparité dans la composition du cluster.

```{r, echo=FALSE, fig.width=4, fig.asp=0.8}
# Example ggplot
ggplot(data = kmeans_basic_df, aes(x = Cluster)) +
  geom_bar(aes(fill = Diagnosis)) +
  ggtitle("Count of Clusters by Diagnosis") +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_fill_brewer(palette = "Set1")

```

On peut aussi observer ci-dessous la représentation graphique des deux clusters obtenus.

```{r, echo=F, fig.width=4, fig.asp=0.8}
fviz_cluster(kmeans_basic, data = scale(km_dataset[,3:12]), geom = c("point"),ellipse.type = "euclid")
```

Dans cette seconde partie de la génération du modèle de clustering par k-mean, nous allons essayer d'améliorer le modèle que nous avions précédemment obtenu. Pour cela, nous allons augmenter le nombre d'essais de génération du clustering en passant à 100 le nombre de positions aléatoire de départ du K.

```{r, echo=FALSE, fig.width=4, fig.asp=0.8}
# Fancy kmeans
set.seed(123)
kmeans_fancy <- kmeans(scale(km_dataset[,3:12]), 2, nstart = 100)

kmeans_fancy_df <- data.frame(Cluster = kmeans_fancy$cluster, km_dataset)

# plot the clusters
fviz_cluster(kmeans_fancy, data = scale(km_dataset[,3:12]), geom = c("point"),ellipse.type = "euclid")
```

On observe pour ce nouveau modèle qu'un des clusters reste majoritairement composé de patients dont la tumeur est maligne tandis que pour le second cluster, la proportion de patient dont la tumeur est maligne diminue comparé au précédent modèle. On peut alors en conclure que le second modèle est plus précis pour partitionner les deux classes de patients. 

```{r, echo=FALSE, fig.width=4, fig.asp=0.8}

ggplot(data = kmeans_fancy_df, aes(x = Cluster)) +
  geom_bar(aes(fill = Diagnosis)) +
  ggtitle("Count of Clusters by Diagnosis") +
  theme(plot.title = element_text(hjust = 0.5))
```

Enfin, dans le tableau ci-dessous, nous réalisons une étude statistique de la composition des clusters.

```{r results='asis', echo=FALSE}
outCtl <- tableby(Cluster ~ Diagnosis + radius_mean + texture_mean + 
                    perimeter_mean + area_mean + smoothness_mean + 
                    compactness_mean + concavity_mean + concave_points_mean + 
                    symmetry_mean + fractal_dimension_mean, 
                  data=kmeans_fancy_df,
                  control=tableby.control(total=T, cat.simplify=F, 
                  numeric.stats = c("Nmiss", "meansd", "range"),digits=1))

summary(outCtl, text=F)
```

## Question 3

Nous allons ensuite développer différents modèles de diagnostic et en évaluer leurs performances. Trois approches vont être utilisées : une méthode par Arbre de décision, une par Forêts aléatoires et une par Ensemble Learning.

### 1 - Méthode par Arbre de décision

Pour cette méthode par Arbre de décision, nous allons commencer par charger une dataset clone des données que nous souhaitons analyser. Nous allons ensuite partitionner ce dataset en deux parties : une qui servira de base de données d'entrainement du modèle, et une qui nous permettra d'évaluer le modèle obtenu précédemment.

```{r, echo=FALSE, message=FALSE}
library(tidyverse)
library(FactoMineR)
library(factoextra)
library(rpart)
library(rpart.plot)
library(caret)

dt_dataset <- clean_data

nb_lignes <- floor((nrow(dt_dataset)*0.75)) #Nombre de lignes de l’échantillon d’apprentissage : 75% du dataset
dt_dataset <- dt_dataset[sample(nrow(dt_dataset)), ] #Ajout de numéros de lignes
dt_dataset.train <- dt_dataset[1:nb_lignes, ] #Echantillon d’apprentissage
dt_dataset.test <- dt_dataset[(nb_lignes+1):nrow(dt_dataset), ] #Echantillon de test
```

Voici ci-dessous les résultats du modèle entraîné sur le dataset d'entraînement. Cet arbre de décision est volumineux et va donc avoir besoin d'un élagage afin de réduire sa complexité. Pour cela, nous allons faire appel à la formule du coût de complexité. Il s'agit d'un paramètre qui permet de contrôler la taille maximale de l'arbre de décision, c'est-à-dire le nombre maximal de nœuds ou de feuilles dans l'arbre.

En général, un arbre de décision plus complexe peut mieux s'adapter aux données d'entraînement, mais il est également plus susceptible de surapprendre (overfitting) et de mal généraliser aux nouvelles données. Par conséquent, le coût de complexité est souvent utilisé pour éviter le surapprentissage en régularisant le modèle et en limitant sa complexité.

```{r echo=FALSE, fig.width=4, fig.asp=0.8}
set.seed(12)
#Construction de l’arbre
dataset.Tree <- rpart(Diagnosis ~ ., 
                      data = dt_dataset.train,
                      method = "class", 
                      control = rpart.control(minsplit = 5, cp=0))

#Affichage du résultat
rpart.plot(dataset.Tree)
```

Pour bien élaguer notre arbre de décision, nous allons chercher le coût de complexité (cp) pour lequel le taux de mauvais classement (xerror) est la plus faible.

```{r echo=FALSE, fig.width=4, fig.asp=0.8}
#On cherche à minimiser l’erreur pour définir le niveau d’élagage
#plotcp(dataset.Tree)
printcp(dataset.Tree)
```

Le coût de complexité optimal pour ce modèle va s'afficher sous ce paragraphe. Ce paramètre va ensuite être utilisé pour élaguer l'arbre obtenu précédemment.

```{r echo=FALSE}
print(dataset.Tree$cptable[which.min(dataset.Tree$cptable[,4]),1])
```

Voici donc ci-dessous le résultat de l'élagage de l'arbre de décision. Ce nouvel arbre est plus court et donc, plus généraliste et aura par conséquence moins tendance à overfitter.

```{r echo=FALSE, fig.width=4, fig.asp=0.8}
set.seed(12)
#Elagage de l’arbre avec le cp optimal
dataset.Tree_Opt <- prune(dataset.Tree,
                          cp = dataset.Tree$cptable[which.min(dataset.Tree$cptable[,4]),1])

#Représentation graphique de l’arbre optimal
rpart.plot(dataset.Tree_Opt)
```

Enfin, nous avons entrainé le modèle obtenu sur le dataset de test que nous avions précédemment créé. Ensuite, nous avons réalisé plusieurs tests statistiques sur les prédictions qu'il a pu faire. 

Les résultats de l'arbre de décision montrent une performance globalement élevée, avec une précision (accuracy) de 91,61%. On peut être satisfait de la capacité du modèle à discriminer entre les classes, avec une sensibilité de 84% pour la classe positive (M) et une spécificité de 95,7% pour la classe négative (B). Le coefficient Kappa de 0,812 indique un accord significatif entre les prédictions du modèle et les valeurs réelles. Ces résultats me donnent confiance dans la compétence du modèle à effectuer des prédictions précises.

```{r echo=FALSE}
#Prédiction du modèle sur les données de test
dataset.test_Predict<-predict(dataset.Tree_Opt,newdata=dt_dataset.test, type= "class")

dt_dataset.test$Diagnosis <- factor(dt_dataset.test$Diagnosis, levels = c("B", "M"))

#Création d'un tableau de confusion
confusionMatrix(dt_dataset.test$Diagnosis, dataset.test_Predict, positive = "M")
```

### 2 - Méthode par Forets aléatoires

```{r echo=FALSE, message=FALSE}
library(readr)
library(randomForest)
library(plotly)

rf_dataset <- clean_data

rf_dataset$Diagnosis <- factor(rf_dataset$Diagnosis)
```

Pour cette méthode par Forêts aléatoires (Random forests), il est nécessaire de partitionner notre dataset initial en deux sections : une d’entraînement et une de test.

```{r}
# Diviser les données en ensembles d'apprentissage et de test
set.seed(123) # pour la reproductibilité des résultats

train_index <- sample(nrow(rf_dataset), 0.7 * nrow(rf_dataset))
train_data <- rf_dataset[train_index, ]
test_data <- rf_dataset[-train_index, ]
```

Nous allons ensuite entrainer le modèle de Random forests. 

```{r echo=FALSE}
# Entraîner le modèle de forêt aléatoire
rf_model <- randomForest(Diagnosis ~ ., data = train_data, ntree = 100, mtry = 2, na.action = na.omit)

# Afficher les résultats du modèle
print(rf_model)
```

Puis faire en sorte d'afficher les variables que le modèle considère comme importantes et indiquer par un score si elles sont plus ou moins importantes.

Les variables les plus importantes sont celles qui ont les valeurs les plus élevées de "MeanDecreaseGini". Dans notre cas, les variables les plus importantes sont "concave_points_mean" avec une valeur de 38.963054, suivie par "area_mean" avec 27.354739 et "perimeter_mean" avec 25.170804.

Les variables ayant des valeurs relativement faibles de "MeanDecreaseGini" sont considérées comme moins importantes pour la prédiction dans ce modèle. Ici, les variables "symmetry_mean" et "fractal_dimension_mean" ont les valeurs les plus faibles avec 3.806719 et 4.201104 respectivement et présentent donc l'importance la plus faible. 

```{r}
# Calculer l'importance des variables
var_importance <- importance(rf_model)

# Afficher les variables les plus importantes
print(var_importance)
```

Enfin nous allons soumettre notre modèle au dataset de test afin qu'il puissent réaliser ses prédictions. Ses résultats seront analysés grace à différents tests statistiques. 

En résumé, notre modèle Random Forest semble présenter de bons résultats avec une précision globale de 91,81%. Il démontre une sensibilité élevée (94,03%) et une bonne spécificité (90,38%). Le coefficient Kappa de 0,8309 indique un accord entre les prédictions du modèle et les étiquettes réelles. 
```{r}
# Évaluer la performance du modèle sur les données de test
rf_predictions <- predict(rf_model, test_data)
confusionMatrix(test_data$Diagnosis, rf_predictions, positive = "M")
```

### 3 - Méthode par Ensemble Learning

```{r}
library(tidyverse)
library(mlbench)
library(caret)
library(caretEnsemble)

el_dataset <- clean_data
```

```{r}
# Example of Boosting Algorithms
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 7
metric <- "Accuracy"

# C5.0
set.seed(seed)
fit.c50 <- train(Diagnosis~., data=clean_data, method="C5.0", metric=metric, trControl=control)

# Stochastic Gradient Boosting
set.seed(seed)
fit.gbm <- train(Diagnosis~., data=clean_data, method="gbm", metric=metric, trControl=control, verbose=FALSE)

# summarize results
boosting_results <- resamples(list(c5.0=fit.c50, gbm=fit.gbm))
summary(boosting_results)
dotplot(boosting_results)
```

```{r}
# Example of Bagging algorithms
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 7
metric <- "Accuracy"

# Bagged CART
set.seed(seed)
fit.treebag <- train(Diagnosis~., data=clean_data, method="treebag", metric=metric, trControl=control)

# Random Forest
set.seed(seed)
fit.rf <- train(Diagnosis~., data=clean_data, method="rf", metric=metric, trControl=control)

# summarize results
bagging_results <- resamples(list(treebag=fit.treebag, rf=fit.rf))
summary(bagging_results)
dotplot(bagging_results)
```

```{r}
# Example of Stacking algorithms
# create submodels
control <- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)
algorithmList <- c('lda', 'rpart', 'glm', 'knn', 'svmRadial')
set.seed(seed)
models <- caretList(Diagnosis~., data=clean_data, trControl=control, methodList=algorithmList)
results <- resamples(models)
summary(results)
dotplot(results)
```

```{r}
# stack using glm
stackControl <- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)
set.seed(seed)
stack.glm <- caretStack(models, method="glm", metric="Accuracy", trControl=stackControl)
print(stack.glm)
```

```{r}
# stack using random forest
set.seed(seed)
stack.rf <- caretStack(models, method="rf", metric="Accuracy", trControl=stackControl)
print(stack.rf)
```
