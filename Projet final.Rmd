---
title: "Projet final Test 2"
output:
  pdf_document: default
  html_document: default
date: "2023-04-24"
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question n°2

### 1 - Analyse descriptive du jeu de donnée par clustering

Pour réaliser cette analyse descriptive, nous allons réaliser une analyse par clustering en utilisant la méthode des k plus proches voisins.

Dans un premier temps, il est alors nécessaire de charger et de nettoyer le jeu de données en renommant les variables et en éliminant les données manquantes. Il est aussi nécessaire de renommer les différentes variables présentes dans le dataset. Ainsi, les variables se terminant par \_mean réfèrent a la moyenne de celle-ci, les variables se terminant par \_SE indiquent la Mean Squared Error et celle terminant par \_worst indiquent la pire évaluation.

```{r, echo=FALSE, message=FALSE}
library(readr)
library(tidyverse)
library(readxl)
library(FactoMineR)
library(factoextra)
library(arsenal)
library(knitr)

data <- read_csv("wdbc.data", 
                 col_names = c("ID number",
                               "Diagnosis",
                               "radius_mean",
                               "texture_mean",
                               "perimeter_mean",
                               "area_mean","smoothness_mean",
                               "compactness_mean",
                               "concavity_mean",
                               "concave_points_mean",
                               "symmetry_mean",
                               "fractal_dimension_mean",
                               "radius_SE","texture_SE",
                               "perimeter_SE","area_SE",
                               "smoothness_SE",
                               "compactness_SE",
                               "concavity_SE",
                               "concave_points_SE",
                               "symmetry_SE",
                               "fractal_dimension_SE",
                               "radius_worst",
                               "texture_worst",
                               "perimeter_worst",
                               "area_worst",
                               "smoothness_worst",
                               "compactness_worst",
                               "concavity_worst",
                               "concave_points_worst",
                               "symmetry_worst",
                               "fractal_dimension_worst"))

clean_data <- data %>% 
  select(c(contains("_mean"), Diagnosis)) %>% 
  drop_na()

km_dataset <- data %>% 
  drop_na()
```

Le paramétrage de la seed ci-dessous permet d'obtenir une reproductibilité dans les résultats obtenus lors des entraînements de modèles.

```{r}
set.seed(123)
```

Par choix, nous allons choisir de nous concentrer sur les variables terminant par \_mean afin d'obtenir une certaine constance des les résultats et leurs interprétabilité.

Afin de déterminer le nombre optimal de clusters, nous allons utiliser l'Elbow method qui consiste a tracer la somme des carrés des distances intra-cluster (WCSS) en fonction du nombre de clusters et à rechercher le point où la courbe forme un "coude" ou un changement de direction brusque.

Ainsi dans la courbe obtenue ci-dessous, on peut observer que la cassure a lieu pour 2 clusters. Voici donc notre nombre optimal de clusters.

```{r, echo=FALSE, fig.width=4, fig.asp=0.8}
# Fancy K-Means
fviz_nbclust(scale(km_dataset[,3:12]), kmeans, nstart=100, method = "wss") + 
  geom_vline(xintercept = 2, linetype = 1)
```

La méthode utilisée ici est que le modèles va créer une dataset clone de celui initial et va y ajouter une variable supplémentaire nommée 'Cluster' et pour chaque ligne y indiquer dans quels cluster elle se trouve.

```{r tidy=TRUE}
kmeans_basic <- kmeans(km_dataset[,3:12], centers = 2)
kmeans_basic_table <- data.frame(kmeans_basic$size, kmeans_basic$centers)
kmeans_basic_df <- data.frame(Cluster = kmeans_basic$cluster, km_dataset)

kable(kmeans_basic_df[1:6, 1:7], 
      format = "latex",
      booktabs = T)
```

On peut aalors représenter les deux clusters créés puis y indiquer en leurs seins s'il s'agit du diagnostic bénin (B) ou alors malin (M).

Dans ce graphique on peut alors observer que le cluster 2 est quasiment totalement constitué de patients dont la tumeur a été diagnostiquée comme maligne tandis que pour le cluster 1, on peut remarquer qu'il y a une nette disparité dans la composition du cluster. 

```{r, echo=FALSE, fig.width=4, fig.asp=0.8}
# Example ggplot
ggplot(data = kmeans_basic_df, aes(x = Cluster)) +
  geom_bar(aes(fill = Diagnosis)) +
  ggtitle("Count of Clusters by Diagnosis") +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_fill_brewer(palette = "Set1")

```

On peut aussi observer ci dessous la représentation graphique des deux clusters obtenus. 

```{r, echo=F, fig.width=4, fig.asp=0.8}
fviz_cluster(kmeans_basic, data = scale(km_dataset[,3:12]), geom = c("point"),ellipse.type = "euclid")
```

Dans cette seconde partie de la génération du modèle de clustering par k-mean, nous allons essayer d'améliorer le modèle que nous avions précédemment obtenu. 
Pour cela, nous allons augmenter le nombre d'éssais de génération du clustering en passant à 100 le nombre de positions aléatoire de départ du K. 

```{r, echo=FALSE, fig.width=4, fig.asp=0.8}
# Fancy kmeans
set.seed(123)
kmeans_fancy <- kmeans(scale(km_dataset[,3:12]), 2, nstart = 100)

kmeans_fancy_df <- data.frame(Cluster = kmeans_fancy$cluster, km_dataset)

# plot the clusters
fviz_cluster(kmeans_fancy, data = scale(km_dataset[,3:12]), geom = c("point"),ellipse.type = "euclid")
```

On observe pour ce nouveau modèle qu'un des clusters reste majoritairement composé de patients dont la tumeur est maligne tandis que pour le second cluster, la proportion de patient dont la tumeur est maligne diminue comparé au précédent modèle.
On peut alors en conclure que le second modèle est plus précis pour partitionner les deux classes de patients 

```{r, echo=FALSE, fig.width=4, fig.asp=0.8}

ggplot(data = kmeans_fancy_df, aes(x = Cluster)) +
  geom_bar(aes(fill = Diagnosis)) +
  ggtitle("Count of Clusters by Diagnosis") +
  theme(plot.title = element_text(hjust = 0.5))
```

Enfin, dans le tableau ci-dessous, nous réalisons une étude statistique de la composition des clusters. 

```{r results='asis', echo=FALSE}
outCtl <- tableby(Cluster ~ Diagnosis + radius_mean + texture_mean + 
                    perimeter_mean + area_mean + smoothness_mean + 
                    compactness_mean + concavity_mean + concave_points_mean + 
                    symmetry_mean + fractal_dimension_mean, 
                  data=kmeans_fancy_df,
                  control=tableby.control(total=T, cat.simplify=F, 
                  numeric.stats = c("Nmiss", "meansd", "range"),digits=1))

summary(outCtl, text=F)
```


## Question 3

Nous allons ensuite développer différents modèles de diagnostic et en évaluer leurs performances. 
Trois approches vont être utilisées : une méthode par Arbre de décision, une par Forêts aléatoires et une par Ensemble Learning.

### 1 - Méthode par Arbre de décision

Pour cette méthode par Arbre de décision, nous allons commencer par charger une dataset clone des données que nous souhaitons analyser. Nous allons ensuite partitionner ce dataset en deux parties : une qui servira de base de données d'entrainement du modèle, et une qui nous permettra d'évaluer le modèle obtenu précédemment. 

```{r, echo=FALSE, message=FALSE}
library(tidyverse)
library(FactoMineR)
library(factoextra)
library(rpart)
library(rpart.plot)
library(caret)

dt_dataset <- clean_data

nb_lignes <- floor((nrow(dt_dataset)*0.75)) #Nombre de lignes de l’échantillon d’apprentissage : 75% du dataset
dt_dataset <- dt_dataset[sample(nrow(dt_dataset)), ] #Ajout de numéros de lignes
dt_dataset.train <- dt_dataset[1:nb_lignes, ] #Echantillon d’apprentissage
dt_dataset.test <- dt_dataset[(nb_lignes+1):nrow(dt_dataset), ] #Echantillon de test
```

Voici ci-dessous les résultats du modèles entraîné sur le dataset d’entraînement. 
Cet arbre de décision est volumineux et va donc avoir besoin d'un élagage afin de réduire se complexité. Pour cela nous allons faire appel à la formule du coût de complexité. Il s'agit d'un paramètre qui permet de contrôler la taille maximale de l'arbre de décision, c'est-à-dire le nombre maximal de nœuds ou de feuilles dans l'arbre. 

En général, un arbre de décision plus complexe peut mieux s'adapter aux données d'entraînement, mais il est également plus susceptible de surapprendre (overfitting) et de mal généraliser aux nouvelles données. Par conséquent, le coût de complexité est souvent utilisé pour éviter le surapprentissage en régularisant le modèle et en limitant sa complexité.

```{r}
set.seed(12)
#Construction de l’arbre
dataset.Tree <- rpart(Diagnosis ~ ., 
                      data = dt_dataset.train,
                      method = "class", 
                      control = rpart.control(minsplit = 5, cp=0))

#Affichage du résultat
rpart.plot(dataset.Tree)
```
Pour bien élager notre arbre de décision, nous allons chercher le coût de compléxité (cp) pour lequel le taux de mauvais classement (xerror) est la plus faible. 

```{r}
#On cherche à minimiser l’erreur pour définir le niveau d’élagage
plotcp(dataset.Tree)
printcp(dataset.Tree)
```

Le coût de complexité optimal pour ce modèle va s'afficher sous ce paragraphe. Ce paramètre va ensuite être utilisé pour élager l'arbre obtenu précédemment.  

```{r}
print(dataset.Tree$cptable[which.min(dataset.Tree$cptable[,4]),1])
```

```{r}
set.seed(12)
#Elagage de l’arbre avec le cp optimal
dataset.Tree_Opt <- prune(dataset.Tree,
                          cp = dataset.Tree$cptable[which.min(dataset.Tree$cptable[,4]),1])

#Représentation graphique de l’arbre optimal
rpart.plot(dataset.Tree_Opt)
```

```{r}
#Prédiction du modèle sur les données de test
dataset.test_Predict<-predict(dataset.Tree_Opt,newdata=dt_dataset.test, type= "class")

dt_dataset.test$Diagnosis <- factor(dt_dataset.test$Diagnosis, levels = c("B", "M"))

#Création d'un tableau de confusion
confusionMatrix(dt_dataset.test$Diagnosis, dataset.test_Predict, positive = "M")

```

### 2 - Méthode par Forets aléatoires

```{r}
library(readr)
library(randomForest)
library(plotly)

rf_dataset <- clean_data

rf_dataset$Diagnosis <- factor(rf_dataset$Diagnosis)
```

```{r}
# Diviser les données en ensembles d'apprentissage et de test
set.seed(123) # pour la reproductibilité des résultats

train_index <- sample(nrow(rf_dataset), 0.7 * nrow(rf_dataset))
train_data <- rf_dataset[train_index, ]
test_data <- rf_dataset[-train_index, ]
```

```{r}
# Entraîner le modèle de forêt aléatoire
rf_model <- randomForest(Diagnosis ~ ., data = train_data, ntree = 100, mtry = 2, na.action = na.omit)

# Afficher les résultats du modèle
print(rf_model)
```

```{r}
# Calculer l'importance des variables
var_importance <- importance(rf_model)

# Afficher les variables les plus importantes
print(var_importance)
```

```{r}
# Évaluer la performance du modèle sur les données de test
rf_predictions <- predict(rf_model, test_data)
confusion_matrix <- table(rf_predictions, test_data$Diagnosis)
accuracy <- sum(diag(confusion_matrix))/sum(confusion_matrix)

# Afficher la matrice de confusion et la précision
print(confusion_matrix)
```

```{r}
# Calcul pour la classe M
precision_M <- confusion_matrix[2,2] / sum(confusion_matrix[,2])
recall_M <- confusion_matrix[2,2] / sum(confusion_matrix[2,])
f1_score_M <- 2 * (precision_M * recall_M) / (precision_M + recall_M)

# Calcul pour la classe B
precision_B <- confusion_matrix[1,1] / sum(confusion_matrix[,1])
recall_B <- confusion_matrix[1,1] / sum(confusion_matrix[1,])
f1_score_B <- 2 * (precision_B * recall_B) / (precision_B + recall_B)

# Affichage des résultats
print(paste("F1-score pour la classe Malin :", f1_score_M))
print(paste("F1-score pour la classe Bénin :", f1_score_B))
```

### 3 - Méthode par Ensemble Learning

```{r}
library(tidyverse)
library(mlbench)
library(caret)
library(caretEnsemble)

el_dataset <- clean_data
```

```{r}
# Example of Boosting Algorithms
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 7
metric <- "Accuracy"

# C5.0
set.seed(seed)
fit.c50 <- train(Diagnosis~., data=clean_data, method="C5.0", metric=metric, trControl=control)

# Stochastic Gradient Boosting
set.seed(seed)
fit.gbm <- train(Diagnosis~., data=clean_data, method="gbm", metric=metric, trControl=control, verbose=FALSE)

# summarize results
boosting_results <- resamples(list(c5.0=fit.c50, gbm=fit.gbm))
summary(boosting_results)
dotplot(boosting_results)
```

```{r}
# Example of Bagging algorithms
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 7
metric <- "Accuracy"

# Bagged CART
set.seed(seed)
fit.treebag <- train(Diagnosis~., data=clean_data, method="treebag", metric=metric, trControl=control)

# Random Forest
set.seed(seed)
fit.rf <- train(Diagnosis~., data=clean_data, method="rf", metric=metric, trControl=control)

# summarize results
bagging_results <- resamples(list(treebag=fit.treebag, rf=fit.rf))
summary(bagging_results)
dotplot(bagging_results)
```

```{r}
# Example of Stacking algorithms
# create submodels
control <- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)
algorithmList <- c('lda', 'rpart', 'glm', 'knn', 'svmRadial')
set.seed(seed)
models <- caretList(Diagnosis~., data=clean_data, trControl=control, methodList=algorithmList)
results <- resamples(models)
summary(results)
dotplot(results)
```

```{r}
# stack using glm
stackControl <- trainControl(method="repeatedcv", number=10, repeats=3, savePredictions=TRUE, classProbs=TRUE)
set.seed(seed)
stack.glm <- caretStack(models, method="glm", metric="Accuracy", trControl=stackControl)
print(stack.glm)
```

```{r}
# stack using random forest
set.seed(seed)
stack.rf <- caretStack(models, method="rf", metric="Accuracy", trControl=stackControl)
print(stack.rf)
```
